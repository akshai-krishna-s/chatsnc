import chromadb
from llama_index.indices import VectorStoreIndex
from llama_index.vector_stores import ChromaVectorStore
from llama_index.storage.storage_context import StorageContext


from ...config import settings
import os

os.environ["OPENAI_API_KEY"] = settings.openai_api_key

db = chromadb.PersistentClient(path="./app/engine/chatsnc/chroma_db")

# get collection
chroma_collection = db.get_collection("sn_college")

# assign chroma as the vector_store to the context
vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
storage_context = StorageContext.from_defaults(vector_store=vector_store)

# load your index from stored vectors
index = VectorStoreIndex.from_vector_store(
    vector_store,
    storage_context=storage_context,
)


def chat(query, chat_history):
    """
    Perform a chat using the given query and chat history.

    Args:
        query (str): The query to be used for the chat.
        chat_history (list): The chat history as a list.

    Returns:
        str: The response generated by the chat engine.
    """
    global index
    chat_engine = index.as_chat_engine(
        streaming=True,
        chat_mode="condense_plus_context",  # type: ignore
        context_prompt=(
            "You are ChatSNC, a chatbot developed by Abil Asok, Indrajith R, Akshai Krishna S and Abhinav Manoj, you can talk"
            " about SN College. Do not answer if the question is not about SN College or the answer to the specific question is not in the context. Do not hallucinate."
            "Here are the relevant documents for the context:\n"
            "{context_str}"
            "\nInstruction: Use the previous chat history, or the context above, to interact and help the user."
        ),
        verbose=False,
        chat_history=chat_history,
    )

    return chat_engine.stream_chat(query)
