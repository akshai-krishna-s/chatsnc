import chromadb
from llama_index.indices import VectorStoreIndex
from llama_index.vector_stores import ChromaVectorStore
from llama_index.storage.storage_context import StorageContext
from llama_index import ServiceContext, set_global_service_context
from llama_index.llms import Cohere
from llama_index.embeddings import OpenAIEmbedding

from llama_index.llms import Ollama

from ...config import settings
import os

os.environ["OPENAI_API_KEY"] = settings.openai_api_key

db = chromadb.PersistentClient(path="./app/engine/chatsnc/chroma_db")

# get collection
chroma_collection = db.get_collection("sn_college")

# assign chroma as the vector_store to the context
vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
storage_context = StorageContext.from_defaults(vector_store=vector_store)

# load your index from stored vectors
index = VectorStoreIndex.from_vector_store(
    vector_store,
    storage_context=storage_context,
)


def chat(query, chat_history):
    """
    Perform a chat using the given query and chat history.

    Args:
        query (str): The query to be used for the chat.
        chat_history (list): The chat history as a list.

    Returns:
        str: The response generated by the chat engine.
    """
    global index
    chat_engine = index.as_chat_engine(
        streaming=True,
        chat_mode="condense_plus_context",  # type: ignore
        context_prompt=(
            "You are a chatbot who can talk"
            " about SN College. Do not answer if the question is not about SN College or the answer is not in the context."
            "Here are the relevant documents for the context:\n"
            "{context_str}"
            "\nInstruction: Use the previous chat history, or the context above, to interact and help the user."
        ),
        verbose=False,
    )

    return chat_engine.stream_chat(query, chat_history=chat_history)
